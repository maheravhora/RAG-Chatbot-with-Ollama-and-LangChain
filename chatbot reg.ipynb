{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4652bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA: Training Generalized Multi-Query Transformer Models from\n",
      "Multi-Head Checkpoints\n",
      "Joshua Ainslie∗, James Lee-Thorp ∗, Michiel de Jong ∗ † †\n",
      "Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai\n",
      "Google Research\n",
      "Abstract\n",
      "Multi-query attention (MQA), which only uses\n",
      "a single key-value head, drastically speeds up\n",
      "decoder inference. However, MQA can lead to\n",
      "quality degradation, and moreover it may not\n",
      "be desirable to train a separate model just for\n",
      "faster inference. We (1) propose a recipe for\n",
      "uptraining existing multi-head language model\n",
      "checkpoints into models with MQA using 5%\n",
      "of original pre-training compute, and (2) intro-\n",
      "duce grouped-query attention (GQA), a gener-\n",
      "alization of multi-query attention which uses\n",
      "an intermediate (more than one, less than num-\n",
      "ber of query heads) number of key-value heads.\n",
      "We show that uptrained GQA achieves quality\n",
      "close to multi-head attention with comparable\n",
      "speed to MQA.\n",
      "1 Introduction\n",
      "Autoregressive decoder inference is a severe bottle-\n",
      "neck for Trans\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2305.13245.pdf\")\n",
    "data = loader.load()\n",
    "print(data[0].page_content[:1000])  # Print the first 1000 characters of the first page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af18dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='GQA: Training Generalized Multi-Query Transformer Models from\\nMulti-Head Checkpoints\\nJoshua Ainslie∗, James Lee-Thorp ∗, Michiel de Jong ∗ † †\\nYury Zemlyanskiy, Federico Lebrón, Sumit Sanghai\\nGoogle Research\\nAbstract\\nMulti-query attention (MQA), which only uses\\na single key-value head, drastically speeds up\\ndecoder inference. However, MQA can lead to\\nquality degradation, and moreover it may not\\nbe desirable to train a separate model just for\\nfaster inference. We (1) propose a recipe for\\nuptraining existing multi-head language model\\ncheckpoints into models with MQA using 5%\\nof original pre-training compute, and (2) intro-\\nduce grouped-query attention (GQA), a gener-\\nalization of multi-query attention which uses\\nan intermediate (more than one, less than num-\\nber of query heads) number of key-value heads.\\nWe show that uptrained GQA achieves quality\\nclose to multi-head attention with comparable\\nspeed to MQA.\\n1 Introduction\\nAutoregressive decoder inference is a severe bottle-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='We show that uptrained GQA achieves quality\\nclose to multi-head attention with comparable\\nspeed to MQA.\\n1 Introduction\\nAutoregressive decoder inference is a severe bottle-\\nneck for Transformer models due to the memory\\nbandwidth overhead from loading decoder weights\\nand all attention keys and values at every decod-\\ning step (Shazeer, 2019; Pope et al., 2022; de Jong\\net al., 2022). The memory bandwidth from loading\\nkeys and values can be sharply reduced through\\nmulti-query attention(Shazeer, 2019), which uses\\nmultiple query heads but single key and value\\nheads.\\nHowever, multi-query attention (MQA) can lead\\nto quality degradation and training instability, and\\nit may not be feasible to train separate models\\noptimized for quality and inference. Moreover,\\nwhile some language models already use multi-\\nquery attention, such as PaLM (Chowdhery et al.,\\n2022), many do not, including publicly available\\nlanguage models such as T5 (Raffel et al., 2020)\\nand LLaMA (Touvron et al., 2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='query attention, such as PaLM (Chowdhery et al.,\\n2022), many do not, including publicly available\\nlanguage models such as T5 (Raffel et al., 2020)\\nand LLaMA (Touvron et al., 2023).\\nThis work contains two contributions for faster\\ninference with large language models. First, we\\n∗Equal contribution.\\n†University of Southern California. Work done at Google\\nResearch.\\nshow that language model checkpoints with multi-\\nhead attention (MHA ) can be uptrained (Komat-\\nsuzaki et al., 2022) to use MQA with a small frac-\\ntion of original training compute. This presents a\\ncost-effective method to obtain fast multi-query as\\nwell as high-quality MHA checkpoints.\\nSecond, we propose grouped-query attention\\n(GQA ), an interpolation between multi-head and\\nmulti-query attention with single key and value\\nheads per subgroup of query heads. We show that\\nuptrained GQA achieves quality close to multi-\\nhead attention while being almost as fast as multi-\\nquery attention.\\n2 Method\\n2.1 Uptraining'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='heads per subgroup of query heads. We show that\\nuptrained GQA achieves quality close to multi-\\nhead attention while being almost as fast as multi-\\nquery attention.\\n2 Method\\n2.1 Uptraining\\nGenerating a multi-query model from a multi-head\\nmodel takes place in two steps: first, converting the\\ncheckpoint, and second, additional pre-training to\\nallow the model to adapt to its new structure. Fig-\\nure 1 shows the process for converting a multi-head\\ncheckpoint into a multi-query checkpoint. The pro-\\njection matrices for key and value heads are mean\\npooled into single projection matrices, which we\\nfind works better than selecting a single key and\\nvalue head or randomly initializing new key and\\nvalue heads from scratch.\\nFigure 1: Overview of conversion from multi-head to\\nmulti-query attention. Key and value projection matri-\\nces from all heads are mean pooled into a single head.\\nThe converted checkpoint is then pre-trained for\\narXiv:2305.13245v3  [cs.CL]  23 Dec 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Figure 2: Overview of grouped-query method. Multi-head attention has H query, key, and value heads. Multi-query\\nattention shares single key and value heads across all query heads. Grouped-query attention instead shares single\\nkey and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\\na small proportion α of its original training steps\\non the same pre-training recipe.\\n2.2 Grouped-query attention\\nGrouped-query attention divides query heads into\\nG groups, each of which shares a single key head\\nand value head. GQA -G refers to grouped-query\\nwith G groups. GQA -1, with a single group and\\ntherefore single key and value head, is equivalent to\\nMQA, while GQA-H, with groups equal to number\\nof heads, is equivalent to MHA . Figure 2 shows a\\ncomparison of grouped-query attention and multi-\\nhead/multi-query attention. When converting a\\nmulti-head checkpoint to a GQA checkpoint, we\\nconstruct each group key and value head by mean-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='comparison of grouped-query attention and multi-\\nhead/multi-query attention. When converting a\\nmulti-head checkpoint to a GQA checkpoint, we\\nconstruct each group key and value head by mean-\\npooling all the original heads within that group.\\nAn intermediate number of groups leads to an\\ninterpolated model that is higher quality thanMQA\\nbut faster than MHA , and, as we will show, rep-\\nresents a favorable trade-off. Going from MHA\\nto MQA reduces H key and value heads to a sin-\\ngle key and value head, reducing the size of the\\nkey-value cache and therefore amount of data that\\nneeds to be loaded by a factor of H. However,\\nlarger models generally scale the number of heads,\\nsuch that multi-query attention represents a more\\naggressive cut in both memory bandwidth and ca-\\npacity. GQA lets us keep the same proportional\\ndecrease in bandwidth and capacity as model size\\nincreases.\\nMoreover, larger models suffer relatively less\\nfrom memory bandwidth overhead from attention,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='decrease in bandwidth and capacity as model size\\nincreases.\\nMoreover, larger models suffer relatively less\\nfrom memory bandwidth overhead from attention,\\nas the KV-cache scales with model dimension\\nwhile model FLOPs and parameters scale with the\\nsquare of model dimension. Finally, standard shard-\\ning for large models replicates the single key and\\nvalue head by the number of model partitions (Pope\\net al., 2022); GQA removes the waste from such\\npartitioning. Therefore, we expect GQA to present\\na particularly good trade-off for larger models.\\nWe note that GQA is not applied to the encoder\\nself-attention layers; encoder representations are\\ncomputed in parallel, and memory bandwidth is\\ntherefore generally not the primary bottleneck.\\n3 Experiments\\n3.1 Experimental setup\\nConfigurations All models are based on the\\nT5.1.1 architecture (Raffel et al., 2020), im-\\nplemented with JAX (Bradbury et al., 2018),\\nFlax (Heek et al., 2020), and Flaxformer 1. For'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Configurations All models are based on the\\nT5.1.1 architecture (Raffel et al., 2020), im-\\nplemented with JAX (Bradbury et al., 2018),\\nFlax (Heek et al., 2020), and Flaxformer 1. For\\nour main experiments we consider T5 Large and\\nXXL with multi-head attention, as well as up-\\ntrained versions of T5 XXL with multi-query and\\ngrouped-query attention. We use the Adafactor op-\\ntimizer with the same hyperparameters and learn-\\ning rate schedule as T5 (Raffel et al., 2020). We\\napply MQA and GQA to decoder self-attention\\nand cross-attention, but not encoder self-attention.\\nUptraining Uptrained models are initialized\\nfrom public T5.1.1 checkpoints. The key and value\\nheads are mean-pooled to the appropriate MQA or\\nGQA structure, and then pre-trained for a further\\nα proportion of original pre-training steps with the\\noriginal pre-training setup and dataset from (Raffel\\net al., 2020). For α = 0.05, training took approxi-\\nmately 600 TPUv3 chip-days.\\nData We evaluate on summarization datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='original pre-training setup and dataset from (Raffel\\net al., 2020). For α = 0.05, training took approxi-\\nmately 600 TPUv3 chip-days.\\nData We evaluate on summarization datasets\\nCNN/Daily Mail (Nallapati et al., 2016), arXiv\\nand PubMed (Cohan et al., 2018), MediaSum (Zhu\\net al., 2021), and Multi-News (Fabbri et al., 2019);\\n1https://github.com/google/flaxformer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Model Tinfer Average CNN arXiv PubMed MediaSum MultiNews WMT TriviaQA\\ns R1 R1 R1 R1 R1 BLEU F1\\nMHA-Large 0.37 46.0 42.9 44.6 46.2 35.5 46.6 27.7 78.2\\nMHA-XXL 1.51 47.2 43.8 45.6 47.5 36.4 46.9 28.4 81.9\\nMQA-XXL 0.24 46.6 43.0 45.0 46.9 36.1 46.5 28.5 81.3\\nGQA-8-XXL 0.28 47.1 43.5 45.4 47.7 36.3 47.2 28.4 81.6\\nTable 1: Inference time and average dev set performance comparison of T5 Large and XXL models with multi-head\\nattention, and 5% uptrained T5-XXL models with multi-query and grouped-query attention on summarization\\ndatasets CNN/Daily Mail, arXiv, PubMed, MediaSum, and MultiNews, translation dataset WMT, and question-\\nanswering dataset TriviaQA.\\ntranslation dataset WMT 2014 English-to-German;\\nand question answering dataset TriviaQA (Joshi\\net al., 2017). We do not evaluate on popular clas-\\nsification benchmarks such as GLUE (Wang et al.,\\n2019) as autoregressive inference is less applicable\\nfor those tasks.\\nFine-tuning For fine-tuning, we use a constant'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='sification benchmarks such as GLUE (Wang et al.,\\n2019) as autoregressive inference is less applicable\\nfor those tasks.\\nFine-tuning For fine-tuning, we use a constant\\nlearning rate of 0.001, batch size 128, and dropout\\nrate 0.1 for all tasks. CNN/Daily Mail and WMT\\nuse input length of 512 and output length 256.\\nOther summarization datasets use input length\\n2048 and output length 512. Finally, TriviaQA\\nuses input length 2048 and output length 32. We\\ntrain until convergence and select the checkpoint\\nwith the highest dev performance. We use greedy\\ndecoding for inference.\\nTiming We report time per sample per TPUv4\\nchip, as measured by xprof (Google, 2020). For\\ntiming experiments we use 8 TPUs with the largest\\nbatch size that fits up to 32 per TPU, and paral-\\nlelization optimized separately for each model.\\n3.2 Main results\\nFigure 3 shows average performance over all\\ndatasets as a function of average inference time\\nfor MHA T5-Large and T5-XXL, and uptrained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='3.2 Main results\\nFigure 3 shows average performance over all\\ndatasets as a function of average inference time\\nfor MHA T5-Large and T5-XXL, and uptrained\\nMQA and GQA -8 XXL models with uptraining\\nproportion α = 0.05. We see that a larger up-\\ntrained MQA model provides a favorable trade-\\noff relative to MHA models, with higher quality\\nand faster inference than MHA -Large. Moreover,\\nGQA achieves significant additional quality gains,\\nachieving performance close to MHA -XXL with\\nspeed close to MQA . Table 1 contains full results\\nfor all datasets.\\n3.3 Ablations\\nThis section presents experiments to investigate\\nthe effect of different modeling choices. We eval-\\n0 0.5 1 1.5\\n46\\n46.5\\n47\\nMHA-Large\\nMHA-XXL\\nMQA-XXL\\nGQA-XXL\\nTime per sample (ms)\\nPerformance\\nFigure 3: Uptrained MQA yields a favorable tradeoff\\ncompared to MHA with higher quality and faster\\nspeed than MHA -Large, and GQA achieves even\\nbetter performance with similar speed gains and\\ncomparable quality to MHA -XXL. Average perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='compared to MHA with higher quality and faster\\nspeed than MHA -Large, and GQA achieves even\\nbetter performance with similar speed gains and\\ncomparable quality to MHA -XXL. Average perfor-\\nmance on all tasks as a function of average inference\\ntime per sample for T5-Large and T5-XXL with multi-\\nhead attention, and 5% uptrained T5-XXL with MQA\\nand GQA-8 attention.\\nuate performance on a representive subsample of\\ntasks: CNN/Daily Mail, (short-form summariza-\\ntion), MultiNews (long-form summarization), and\\nTriviaQA (question-answering).\\nCheckpoint conversion Figure 4 compares the\\nperformance of different methods for checkpoint\\nconversion. Mean pooling appears to work best,\\nfollowed by selecting a single head and then ran-\\ndom initialization. Intuitively, results are ordered\\nby the degree to which information is preserved\\nfrom the pre-trained model.\\nUptraining steps Figure 5 shows how perfor-\\nmance varies with uptraining proportion for T5\\nXXL with MQA and GQA . First, we note that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='from the pre-trained model.\\nUptraining steps Figure 5 shows how perfor-\\nmance varies with uptraining proportion for T5\\nXXL with MQA and GQA . First, we note that\\nGQA already achieves reasonable performance af-\\nter conversion while MQA requires uptraining to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='54.4 54.6 54.8 55 55.2 55.4 55.6\\nMean\\nFirst\\nRandom\\nFigure 4: Performance comparison of different check-\\npoint conversion methods for T5-Large uptrained to\\nMQA with proportion α = 0.05. ‘Mean’ mean-pools\\nkey and value heads, ‘First’ selects the first head and\\n‘Random’ initializes heads from scratch.\\nbe useful. Both MQA and GQA gain from 5%\\nuptraining with diminishing returns from 10%.\\n0 0.02 0.04 0.06 0.08 0.1\\n54\\n55\\n56\\n57\\nUptraining proportion α\\nPerformance\\nMHA\\nGQA\\nMQA\\nFigure 5: Performance as a function of uptraining pro-\\nportion for T5 XXL models with MQA and GQA-8.\\nNumber of groups Figure 6 demonstrates the\\neffect of the number of GQA groups on infer-\\nence speed. For larger models the memory band-\\nwidth overhead from the KV cache is less con-\\nstraining (Shazeer, 2019), while the reduction in\\nkey-value size is sharper due to the increased num-\\nber of heads. As a result, increasing the number\\nof groups from MQA only results in modest slow-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='straining (Shazeer, 2019), while the reduction in\\nkey-value size is sharper due to the increased num-\\nber of heads. As a result, increasing the number\\nof groups from MQA only results in modest slow-\\ndowns initially, with increasing cost as we move\\ncloser to MHA . We selected 8 groups as a favor-\\nable middle ground.\\n4 Related Work\\nThis work is focused on achieving a better trade-\\noff between decoder quality and inference time\\nthrough reducing the memory bandwidth over-\\nhead (Williams et al., 2009) from loading keys\\nand values. Shazeer (2019) first proposed reduc-\\ning this overhead through multi-query attention.\\nFollow-up work showed that multi-query attention\\n1 4 8 16 32 64\\n1\\n2\\nGQA groups\\nTime per sample (s)\\nMHA\\nGQA\\nMQA\\nFigure 6: Time per sample for GQA-XXL as a function\\nof the number of GQA groups with input length 2048\\nand output length 512. Going from 1 ( MQA ) to 8\\ngroups adds modest inference overhead, with increasing\\ncost to adding more groups.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='of the number of GQA groups with input length 2048\\nand output length 512. Going from 1 ( MQA ) to 8\\ngroups adds modest inference overhead, with increasing\\ncost to adding more groups.\\nis especially helpful for long inputs (Pope et al.,\\n2022; de Jong et al., 2022). Rabe (2023) indepen-\\ndently developed GQA with public implementa-\\ntion. Other works have explored grouping atten-\\ntion heads for computational efficiency (Park et al.,\\n2020; Luo et al., 2022; Ni et al., 2023) without\\nfocusing specifically on key-value heads, which\\ndetermine memory bandwidth overhead.\\nA number of other methods have been proposed\\nto reduce memory bandwidth overhead from keys\\nand values, as well as parameters. Flash atten-\\ntion (Dao et al., 2022) structures the attention com-\\nputation to avoid materializing the quadratic at-\\ntention scores, reducing memory and speeding up\\ntraining. Quantization (Dettmers et al., 2022; Fran-\\ntar et al., 2022) reduces the size of weights and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='tention scores, reducing memory and speeding up\\ntraining. Quantization (Dettmers et al., 2022; Fran-\\ntar et al., 2022) reduces the size of weights and\\nactivations, including keys and values, by lowering\\nprecision. Model distillation (Hinton et al., 2015;\\nGou et al., 2021) instead reduces model size at\\na given precision, using data generated from the\\nlarger model to finetune the smaller model. Layer-\\nsparse cross-attention (de Jong et al., 2022) elim-\\ninates most cross-attention layers which make up\\nthe primary expense for longer inputs. Speculative\\nsampling (Chen et al., 2023; Leviathan et al., 2022)\\nameliorates the memory bandwidth bottleneck by\\nproposing multiple tokens with a smaller model\\nwhich are then scored in parallel by a larger model.\\nFinally, the uptraining procedure we propose\\nis inspired by Komatsuzaki et al. (2022), which\\nuptrains standard T5 checkpoints into sparsely acti-\\nvated Mixture-of-Experts models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='5 Conclusion\\nLanguage models are expensive for inference pri-\\nmarily due to the memory bandwidth overhead\\nfrom loading keys and values. Multi-query atten-\\ntion reduces this overhead at the cost of decreased\\nmodel capacity and quality. We propose to convert\\nmulti-head attention models to multi-query models\\nwith a small fraction of original pre-training com-\\npute. Moreover, we introduce grouped-query atten-\\ntion, an interpolation of multi-query and multi-head\\nattention that achieves quality close to multi-head\\nat comparable speed to multi-query attention.\\nLimitations\\nThis paper focuses on ameliorating the memory\\nbandwidth overhead from loading keys and values.\\nThis overhead is most important when generating\\nlonger sequences, for which quality is inherently\\ndifficult to evaluate. For summarization we employ\\nRouge score, which we know is a flawed evaluation\\nthat does not tell the whole story; for that reason,\\nit is difficult to be certain our trade-offs are cor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Rouge score, which we know is a flawed evaluation\\nthat does not tell the whole story; for that reason,\\nit is difficult to be certain our trade-offs are cor-\\nrect. Due to limited computation, we also do not\\ncompare our XXL GQA model to a comparitive\\nmodel trained from scratch, so we do not know the\\nrelative performance of uptraining vs training from\\nscratch. Finally, we evaluate the impact of uptrain-\\ning and GQA only on encoder-decoder models.\\nRecently, decoder-only models are extremely pop-\\nular, and since these models do not have separate\\nself-attention and cross-attention, we expect GQA\\nto have a stronger advantage over MQA.\\nAcknowlegements\\nWe thank Santiago Ontañón, Afroz Mohiuddin,\\nWilliam Cohen and others at Google Research for\\ninsightful advice and discussion.\\nReferences\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='References\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irv-\\ning, Jean-Baptiste Lespiau, Laurent Sifre, and\\nJohn Jumper. 2023. Accelerating large language\\nmodel decoding with speculative sampling. CoRR,\\nabs/2302.01318.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Hutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways.\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\\nGoharian. 2018. A discourse-aware attention model\\nfor abstractive summarization of long documents. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Arman Cohan, Franck Dernoncourt, Doo Soon Kim,\\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\\nGoharian. 2018. A discourse-aware attention model\\nfor abstractive summarization of long documents. In\\nProceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 2 (Short Papers), pages 615–621, New Or-\\nleans, Louisiana. Association for Computational Lin-\\nguistics.\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness.\\nCoRR, abs/2205.14135.\\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,\\nNicholas FitzGerald, Sumit Sanghai, Fei Sha, and\\nWilliam Cohen. 2022. FiDO: Fusion-in-decoder opti-\\nmized for stronger performance and faster inference.\\narXiv preprint arXiv:2212.08153.\\nTim Dettmers, Mike Lewis, Younes Belkada, and\\nLuke Zettlemoyer. 2022. Llm.int8(): 8-bit ma-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='mized for stronger performance and faster inference.\\narXiv preprint arXiv:2212.08153.\\nTim Dettmers, Mike Lewis, Younes Belkada, and\\nLuke Zettlemoyer. 2022. Llm.int8(): 8-bit ma-\\ntrix multiplication for transformers at scale. CoRR,\\nabs/2208.07339.\\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and\\nDragomir R. Radev. 2019. Multi-news: A large-scale\\nmulti-document summarization dataset and abstrac-\\ntive hierarchical model. In Proceedings of the 57th\\nConference of the Association for Computational Lin-\\nguistics, ACL 2019, Florence, Italy, July 28- August\\n2, 2019, Volume 1: Long Papers, pages 1074–1084.\\nAssociation for Computational Linguistics.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\\nDan Alistarh. 2022. GPTQ: accurate post-training\\nquantization for generative pre-trained transformers.\\nCoRR, abs/2210.17323.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Google. 2020. Profile your model with cloud tpu\\ntools. https://cloud.google.com/tpu/docs/\\ncloud-tpu-tools. Accessed: 2022-11-11.\\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\\nDacheng Tao. 2021. Knowledge distillation: A sur-\\nvey. Int. J. Comput. Vis., 129(6):1789–1819.\\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\\nand Marc van Zee. 2020. Flax: A neural network\\nlibrary and ecosystem for JAX.\\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\\n2015. Distilling the knowledge in a neural network.\\nCoRR, abs/1503.02531.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='the Association for Computational Linguistics, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,\\nYi Tay, Mostafa Dehghani, and Neil Houlsby. 2022.\\nSparse upcycling: Training mixture-of-experts from\\ndense checkpoints.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\\n2022. Fast inference from transformers via spec-\\nulative decoding. CoRR, abs/2211.17192.\\nGen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan\\nCao, Yongjian Wu, Feiyue Huang, and Rongrong Ji.\\n2022. Towards lightweight transformer via group-\\nwise transformation for vision-and-language tasks.\\nIEEE Trans. Image Process., 31:3386–3398.\\nRamesh Nallapati, Bowen Zhou, Cícero Nogueira dos\\nSantos, Çaglar Gülçehre, and Bing Xiang. 2016.\\nAbstractive text summarization using sequence-to-\\nsequence rnns and beyond. In Proceedings of the\\n20th SIGNLL Conference on Computational Natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Santos, Çaglar Gülçehre, and Bing Xiang. 2016.\\nAbstractive text summarization using sequence-to-\\nsequence rnns and beyond. In Proceedings of the\\n20th SIGNLL Conference on Computational Natural\\nLanguage Learning, CoNLL 2016, Berlin, Germany,\\nAugust 11-12, 2016, pages 280–290. ACL.\\nJinjie Ni, Rui Mao, Zonglin Yang, Han Lei, and Erik\\nCambria. 2023. Finding the pillars of strength for\\nmulti-head attention. In Proceedings of the 61st An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2023,\\nToronto, Canada, July 9-14, 2023, pages 14526–\\n14540. Association for Computational Linguistics.\\nSungrae Park, Geewook Kim, Junyeop Lee, Jun-\\nbum Cha, Ji-Hoon Kim, and Hwalsuk Lee. 2020.\\nScale down transformer by grouping features for\\na lightweight character-level language model. In\\nProceedings of the 28th International Confer-\\nence on Computational Linguistics, COLING 2020,\\nBarcelona, Spain (Online), December 8-13, 2020,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='a lightweight character-level language model. In\\nProceedings of the 28th International Confer-\\nence on Computational Linguistics, COLING 2020,\\nBarcelona, Spain (Online), December 8-13, 2020,\\npages 6883–6893. International Committee on Com-\\nputational Linguistics.\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\\nJacob Devlin, James Bradbury, Anselm Levskaya,\\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\\nJeff Dean. 2022. Efficiently scaling transformer in-\\nference. arXiv preprint arXiv:2211.05102.\\nMarkus Rabe. 2023. Memory-efficient attention.\\nhttps://github.com/google/flaxformer/\\nblob/main/flaxformer/components/\\nattention/memory_efficient_attention.py.\\nAccessed: 2023-05-23.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\nNoam Shazeer. 2019. Fast transformer decoding:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='of transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\nNoam Shazeer. 2019. Fast transformer decoding:\\nOne write-head is all you need. arXiv preprint\\narXiv:1911.02150.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R. Bowman. 2019.\\nGLUE: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In 7th In-\\nternational Conference on Learning Representations,\\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net.\\nSamuel Williams, Andrew Waterman, and David A. Pat-\\nterson. 2009. Roofline: an insightful visual perfor-\\nmance model for multicore architectures. Commun.\\nACM, 52(4):65–76.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='OpenReview.net.\\nSamuel Williams, Andrew Waterman, and David A. Pat-\\nterson. 2009. Roofline: an insightful visual perfor-\\nmance model for multicore architectures. Commun.\\nACM, 52(4):65–76.\\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\\n2021. Mediasum: A large-scale media interview\\ndataset for dialogue summarization. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, NAACL-HLT 2021,\\nOnline, June 6-11, 2021, pages 5927–5934. Associa-\\ntion for Computational Linguistics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-27T01:20:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-27T01:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.13245.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='A Training Stability\\nWe find that multi-query attention can lead to train-\\ning instability during fine-tuning, in particular com-\\nbined with long input tasks. We trained multiple\\nT5-Large models with multi-query attention from\\nscratch. In each case, pre-training suffered from\\nfrequent loss spikes and the final models diverged\\nimmediately when fine-tuning on long-input tasks.\\nUptrained multi-query attention models are more\\nstable but still display high variance, so for multi-\\nquery models on unstable tasks we report average\\nperformance over three fine-tuning runs. Uptrained\\ngrouped-query attention models, however, appear\\nto be stable, so we did not investigate futher on the\\nroot causes of multi-query instability.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chaunk_docs = text_splitter.split_documents(data)\n",
    "chaunk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d7569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x23d3e8d9280>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector embeddings and vector store\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(chaunk_docs[:30], OllamaEmbeddings(model=\"mistral\"))\n",
    "\n",
    "db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d6037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tention scores, reducing memory and speeding up\n",
      "training. Quantization (Dettmers et al., 2022; Fran-\n",
      "tar et al., 2022) reduces the size of weights and\n",
      "activations, including keys and values, by lowering\n",
      "precision. Model distillation (Hinton et al., 2015;\n",
      "Gou et al., 2021) instead reduces model size at\n",
      "a given precision, using data generated from the\n",
      "larger model to finetune the smaller model. Layer-\n",
      "sparse cross-attention (de Jong et al., 2022) elim-\n",
      "inates most cross-attention layers which make up\n",
      "the primary expense for longer inputs. Speculative\n",
      "sampling (Chen et al., 2023; Leviathan et al., 2022)\n",
      "ameliorates the memory bandwidth bottleneck by\n",
      "proposing multiple tokens with a smaller model\n",
      "which are then scored in parallel by a larger model.\n",
      "Finally, the uptraining procedure we propose\n",
      "is inspired by Komatsuzaki et al. (2022), which\n",
      "uptrains standard T5 checkpoints into sparsely acti-\n",
      "vated Mixture-of-Experts models.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main contribution of the paper?\"\n",
    "retrived_result_db= db.similarity_search(query)\n",
    "print(retrived_result_db[0].page_content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant. \n",
    "You will be given a query and some context from a research paper.\n",
    "Your task is to answer the query based on the context provided.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "                                        \n",
    "question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3862dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voram\\AppData\\Local\\Temp\\ipykernel_22120\\698616864.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"mistral\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4feb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nYou are a helpful AI assistant. \\nYou will be given a query and some context from a research paper.\\nYour task is to answer the query based on the context provided.\\n<context>\\n{context}\\n</context>\\n\\nquestion: {input}\\n'), additional_kwargs={})])\n",
       "| Ollama(model='mistral')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chains\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766de936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023D3E8D9280>, search_kwargs={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retriever\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb49171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac522ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, that's correct. According to the context provided, all the models are based on the T5.1.1 architecture.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retrieval_chain.invoke({\"input\": \"All models are based on the T5.1.1 architecture\"})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
